{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "## from week 6 lab\r\n",
    "def dcg_at_k(r, k, method=0):\r\n",
    "    \"\"\"Score is discounted cumulative gain (dcg)\r\n",
    "\r\n",
    "    Relevance is positive real values.  Can use binary\r\n",
    "    as the previous methods.\r\n",
    "\r\n",
    "    Example from\r\n",
    "    http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\r\n",
    "    >>> r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\r\n",
    "    >>> dcg_at_k(r, 1)\r\n",
    "    3.0\r\n",
    "    >>> dcg_at_k(r, 1, method=1)\r\n",
    "    3.0\r\n",
    "    >>> dcg_at_k(r, 2)\r\n",
    "    5.0\r\n",
    "    >>> dcg_at_k(r, 2, method=1)\r\n",
    "    4.2618595071429155\r\n",
    "    >>> dcg_at_k(r, 10)\r\n",
    "    9.6051177391888114\r\n",
    "    >>> dcg_at_k(r, 11)\r\n",
    "    9.6051177391888114\r\n",
    "\r\n",
    "    Args:\r\n",
    "        r: Relevance scores (list or numpy) in rank order\r\n",
    "            (first element is the first item)\r\n",
    "        k: Number of results to consider\r\n",
    "        method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]\r\n",
    "\r\n",
    "    Returns:\r\n",
    "        Discounted cumulative gain\r\n",
    "    \"\"\"\r\n",
    "    import numpy as np\r\n",
    "    r = np.asfarray(r)[:k]\r\n",
    "    if r.size: ## why is this r.size? when will this be false?\r\n",
    "        return r[0] + np.sum(r[1:] / np.log2(np.arange(2, r.size + 1)))\r\n",
    "    return 0."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "## from week 6 lab\r\n",
    "def ndcg_at_k(r, k, method=0):\r\n",
    "    \"\"\"Score is normalized discounted cumulative gain (ndcg)\r\n",
    "\r\n",
    "    Relevance is positive real values.  Can use binary\r\n",
    "    as the previous methods.\r\n",
    "\r\n",
    "    Example from\r\n",
    "    http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\r\n",
    "    >>> r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\r\n",
    "    >>> ndcg_at_k(r, 1)\r\n",
    "    1.0\r\n",
    "    >>> r = [2, 1, 2, 0]\r\n",
    "    >>> ndcg_at_k(r, 4)\r\n",
    "    0.9203032077642922\r\n",
    "    >>> ndcg_at_k(r, 4, method=1)\r\n",
    "    0.96519546960144276\r\n",
    "    >>> ndcg_at_k([0], 1)\r\n",
    "    0.0\r\n",
    "    >>> ndcg_at_k([1], 2)\r\n",
    "    1.0\r\n",
    "\r\n",
    "    Args:\r\n",
    "        r: Relevance scores (list or numpy) in rank order\r\n",
    "            (first element is the first item)\r\n",
    "        k: Number of results to consider\r\n",
    "        method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]\r\n",
    "                If 1 then weights are [1.0, 0.6309, 0.5, 0.4307, ...]\r\n",
    "\r\n",
    "    Returns:\r\n",
    "        Normalized discounted cumulative gain\r\n",
    "    \"\"\"\r\n",
    "    import numpy as np\r\n",
    "\r\n",
    "    dcg_max = dcg_at_k(sorted(r, reverse=True), k, method)\r\n",
    "    if not dcg_max:\r\n",
    "        return 0.\r\n",
    "    # print('For k is {}, DCG scorce is {}'.format(k,dcg_at_k(r, k, method)))\r\n",
    "    # print('For k is {}, IDCG scorce is {}'.format(k,dcg_max))\r\n",
    "    return dcg_at_k(r, k, method) / dcg_max"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "def assigningBM25ScoreToRelevantAndRetrieved(_bm25ScoreDf, relevantDocsList):\r\n",
    "    \"\"\"[summary]\r\n",
    "    This function helps to assign zero values to those non-relevant and retrieved documents\r\n",
    "    It retents the score of those relevant and retrieved\r\n",
    "\r\n",
    "    Args:\r\n",
    "        _bm25ScoreDf ([dataframe]): [a dataframe where rows are modules and columns is the bm25 scores]\r\n",
    "        relevantAndRetrievedDocs ([list]): [list of modules based on the golden standard(idea outcome based on survey)]\r\n",
    "    \"\"\"\r\n",
    "    df = _bm25ScoreDf.copy(deep = False)\r\n",
    "    irrelevantAndRetrievedDocsList = list(set(df.index) - set(relevantDocsList))\r\n",
    "    \r\n",
    "    for relevantAndRetrievedDoc in irrelevantAndRetrievedDocsList:\r\n",
    "        df.loc[relevantAndRetrievedDoc]['bm25Score'] = 0\r\n",
    "    \"\"\"[summary]\r\n",
    "    output is a df with score that are retrieved and relevant(relevant depends on the gold standard)\r\n",
    "    \"\"\"\r\n",
    "    return(df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "def NDCGWithVariousK(retrievedDocsDf,listOfRelevantDocs, exportResults = 0, queryNum = '', fileName = 'test'):\r\n",
    "    \"\"\"[summary]\r\n",
    "    This function compute the NDGC at vaious K\r\n",
    "\r\n",
    "    Args:\r\n",
    "        retrievedDocsDf ([dataframe]): [dataframe of retrieved documents and it's bm25 score]\r\n",
    "        listOfRelevantDocs ([list]): [list of relevant Documents based on gold standard]\r\n",
    "        exportResults (int, optional): [to determine to export ndcg results]. Defaults to 0 and 1 to export ndgc score\r\n",
    "        fileName (str, optional): [fileName to be exported ideally it should be the \"ndcg_score_'model name']. Defaults to 'test'.\r\n",
    "    \"\"\"\r\n",
    "    ## assign zero values to those non-relevant and retrieved documents, It retain the score of those relevant and retrieved\r\n",
    "    BM25ScoreToRelevantAndRetrieved = assigningBM25ScoreToRelevantAndRetrieved(retrievedDocsDf,listOfRelevantDocs)\r\n",
    "    ## obtain the score of the BM25 of the relevant and retrieved modules\r\n",
    "    BM25ScoreToRelevantAndRetrievedScoreList = list(BM25ScoreToRelevantAndRetrieved.bm25Score)\r\n",
    "    \r\n",
    "    ## dict to save NDCGScore ie {k(ranking):NDCG Score}\r\n",
    "    NDCGScoreDict = {}\r\n",
    "    for i in range(1,len(BM25ScoreToRelevantAndRetrievedScoreList)+1):\r\n",
    "        ndcg_at_kScore = ndcg_at_k(BM25ScoreToRelevantAndRetrievedScoreList,i)\r\n",
    "        # print('For k is {}, NDCG scorce is {}\\n'.format(i,ndcg_at_kScore))\r\n",
    "        NDCGScoreDict[i] = ndcg_at_kScore\r\n",
    "    \r\n",
    "    ## convert dict to df for easier sorting analysis of the scores and exporting it to csv\r\n",
    "    import pandas as pd\r\n",
    "    NDCGDf = pd.DataFrame.from_dict(NDCGScoreDict,orient='index',columns=['NDCGScore{}'.format(queryNum)])\r\n",
    "    NDCGDf.reset_index(inplace = True)\r\n",
    "    ## rename the column to k columns \r\n",
    "    NDCGDf.rename(columns={\"index\": \"k\"}, inplace = True)\r\n",
    "    \r\n",
    "    ## to export the ndcg scores to csv if exportResults == 1\r\n",
    "    if exportResults == 1:\r\n",
    "        fileName = 'ndcg_score_{}.csv'.format(fileName)\r\n",
    "        NDCGDf.to_csv('../results/ndcg_score/{}'.format(fileName))\r\n",
    "    return(NDCGDf)\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Toy Problem formulation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "if __name__ == \"__main__\":\r\n",
    "    \"Test Case : the retrievedDocScore\"\r\n",
    "    ## assume docs are not in bm25 scorce order\r\n",
    "    retrievedDocs = ['D','C', 'B','A'] \r\n",
    "    retrievedDocsScore = [0.43, 0.26, 0.03, 0.37]\r\n",
    "    ## I realised that the score should be in ascending order of bm25 score hence I made some changes to fit our use case\r\n",
    "    # retrievedDocsScore = [0.43,  0.37, 0.26, 0.03]\r\n",
    "\r\n",
    "    ## creating a retrievedDocsDf for test cases\r\n",
    "    ## this should be the same format of the bm25 output\r\n",
    "    retrievedDocsDict = {}\r\n",
    "    for index in range(len(retrievedDocs)):\r\n",
    "        retrievedDocsDict[retrievedDocs[index]] = retrievedDocsScore[index]\r\n",
    "    import pandas as pd\r\n",
    "    retrievedDocsDf1 = pd.DataFrame.from_dict(retrievedDocsDict,orient='index',columns = ['bm25Score'])\r\n",
    "\r\n",
    "    print('BM25 output:')\r\n",
    "    retrievedDocsDf1\r\n",
    "\r\n",
    "    \"Test Case : the retrievedDocScore\"\r\n",
    "    ## assume docs are not in bm25 scorce order\r\n",
    "    retrievedDocs = ['C','D', 'B','A'] \r\n",
    "    retrievedDocsScore = [0.5, 0.3, 0.2, 0.1]\r\n",
    "    ## I realised that the score should be in ascending order of bm25 score hence I made some changes to fit our use case\r\n",
    "    # retrievedDocsScore = [0.43,  0.37, 0.26, 0.03]\r\n",
    "\r\n",
    "    ## creating a retrievedDocsDf for test cases\r\n",
    "    ## this should be the same format of the bm25 output\r\n",
    "    retrievedDocsDict = {}\r\n",
    "    for index in range(len(retrievedDocs)):\r\n",
    "        retrievedDocsDict[retrievedDocs[index]] = retrievedDocsScore[index]\r\n",
    "    import pandas as pd\r\n",
    "    retrievedDocsDf2 = pd.DataFrame.from_dict(retrievedDocsDict,orient='index',columns = ['bm25Score'])\r\n",
    "\r\n",
    "    print('BM25 output:')\r\n",
    "    retrievedDocsDf2\r\n",
    "    \"Test Case : The Relevant Docs\"\r\n",
    "    relevantDocs1 = ['B','D','E']\r\n",
    "    print('List of relevant Docs: {}'.format(relevantDocs1))\r\n",
    "    relevantDocs2 = ['A','C']\r\n",
    "    print('List of relevant Docs: {}'.format(relevantDocs2))\r\n",
    "\r\n",
    "    retrievedlist = [retrievedDocsDf1,retrievedDocsDf2]\r\n",
    "    relevantlist =[relevantDocs1,relevantDocs2]\r\n",
    "    \r\n",
    "    ## test case\r\n",
    "    import pandas as pd\r\n",
    "    ## this index is meant to keep track of the NDCG score of each query\r\n",
    "    queryIndex = 0\r\n",
    "    for retrieved in retrievedlist:\r\n",
    "    ## to compute the NDCG of a single query\r\n",
    "        NDCGWithVariousKdf = NDCGWithVariousK(retrieved,relevantlist[queryIndex],0,queryIndex)\r\n",
    "    ## if this is 1st NDCG score been compute, make it's df to NDCG df else merge with the current overall NDGC df\r\n",
    "        if queryIndex == 0:\r\n",
    "            NDCGDf = NDCGWithVariousKdf\r\n",
    "        else:\r\n",
    "            NDCGDf = pd.merge(NDCGDf, NDCGWithVariousKdf, on=[\"k\"])\r\n",
    "        queryIndex += 1"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "BM25 output:\n",
      "BM25 output:\n",
      "List of relevant Docs: ['B', 'D', 'E']\n",
      "List of relevant Docs: ['A', 'C']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "def clean_elective_names(relevant_results):\r\n",
    "    # clean up the relevant course names \r\n",
    "\r\n",
    "    #https://stackoverflow.com/questions/2582138/finding-and-replacing-elements-in-a-list\r\n",
    "    try:\r\n",
    "        relevant_results = relevant_results.split(',')\r\n",
    "        relevant_results = [x.replace(\"'\",'') for x in relevant_results]\r\n",
    "        relevant_results = [x.replace(\"[\",'') for x in relevant_results]\r\n",
    "        relevant_results = [x.replace(\"]\",'') for x in relevant_results]\r\n",
    "    ## this is required as apart from the index 0 module the other modules still retain a space inform of them\r\n",
    "        relevant_results2 = [x.replace(\" \",'',1) for x in relevant_results if x != relevant_results[0]]\r\n",
    "    ## thus the next 2 lines of code help to reinsert the 0th index modules and reassign relevant_results2 to relevant_results\r\n",
    "        relevant_results2.insert(0,relevant_results[0])\r\n",
    "        relevant_results = relevant_results2\r\n",
    "    except:\r\n",
    "        pass\r\n",
    "    replacements = {\r\n",
    "        ' 50.035 Computer Vision': '50.035 Computer Vision'\r\n",
    "        ,'50.043 Database Systems / Database and Big Data Systems (for class 2021)': '50.043 Database Systems'\r\n",
    "        }\r\n",
    "\r\n",
    "    relevant_results = [replacements.get(x, x) for x in relevant_results]\r\n",
    "    \r\n",
    "    if '40.302 Advanced Optim/ 40.305 Advanced Stochastic' in relevant_results:\r\n",
    "        relevant_results.remove('40.302 Advanced Optim/ 40.305 Advanced Stochastic')\r\n",
    "        relevant_results.append('40.302 Advanced Topics in Optimisation#')\r\n",
    "        relevant_results.append('40.305 Advanced Topics in Stochastic Modelling#')\r\n",
    "    return relevant_results"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "## function to compute the NDCG for cosine simliarities for model 1\r\n",
    "def get_NDCG_cosine_no_expan(query_val,tf):\r\n",
    "    import CosineSimilarity_no_query_expan\r\n",
    "    ## compute Cosine simliarities score\r\n",
    "    cosineSimDf = CosineSimilarity_no_query_expan.rankedModuleOfCosineSim(query_val,tf)\r\n",
    "    cosineSimDf = cosineSimDf.T\r\n",
    "\r\n",
    "    ## this section help to compute and obtain the NDCG for each query and store in df\r\n",
    "    import pandas as pd\r\n",
    "    queryCount = 0\r\n",
    "    NDCGDf = 0\r\n",
    "    for query,row in cosineSimDf.iterrows():\r\n",
    "        ## create the df for retrieved docs and it's score\r\n",
    "            retrievedDocsDict = {}\r\n",
    "            cleanedElectives = clean_elective_names(row['topModules'])\r\n",
    "            for index in range(len(row['topModules'])):\r\n",
    "                retrievedDocsDict[cleanedElectives[index]] = row['topModulesScore'][index]\r\n",
    "            import pandas as pd\r\n",
    "            retrievedDocsDf = pd.DataFrame.from_dict(retrievedDocsDict,orient='index',columns = ['bm25Score'])\r\n",
    "        \r\n",
    "        ## cleaned golden/vaildation set modules\r\n",
    "            validModules = clean_elective_names(query_val['expectedElectivesInOrder'][queryCount])\r\n",
    "        ## to compute the NDCG of a single query\r\n",
    "            NDCGWithVariousKdf = NDCGWithVariousK(retrievedDocsDf,validModules,0,queryCount)\r\n",
    "        ## if this is 1st NDCG score been compute, make it's df to NDCG df else merge with the current overall NDGC df\r\n",
    "            if queryCount == 0:\r\n",
    "                NDCGDf = NDCGWithVariousKdf\r\n",
    "            else:\r\n",
    "                NDCGDf = pd.merge(NDCGDf, NDCGWithVariousKdf, on=[\"k\"])\r\n",
    "            queryCount += 1\r\n",
    "            \r\n",
    "    ## return a df with all the ndcg results\r\n",
    "    \r\n",
    "    return(NDCGDf)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "## function to compute the NDCG for cosine simliarities for model 2 and 3\r\n",
    "def get_NDCG_cosine(query_val,tf):\r\n",
    "    import CosineSimilarity\r\n",
    "    ## compute Cosine simliarities score\r\n",
    "    cosineSimDf = CosineSimilarity.rankedModuleOfCosineSim(query_val,tf)\r\n",
    "    cosineSimDf = cosineSimDf.T\r\n",
    "\r\n",
    "    ## this section help to compute and obtain the NDCG for each query and store in df\r\n",
    "    import pandas as pd\r\n",
    "    queryCount = 0\r\n",
    "    NDCGDf = 0\r\n",
    "    for query,row in cosineSimDf.iterrows():\r\n",
    "        ## create the df for retrieved docs and it's score\r\n",
    "            retrievedDocsDict = {}\r\n",
    "            cleanedElectives = clean_elective_names(row['topModules'])\r\n",
    "            for index in range(len(row['topModules'])):\r\n",
    "                retrievedDocsDict[cleanedElectives[index]] = row['topModulesScore'][index]\r\n",
    "            import pandas as pd\r\n",
    "            retrievedDocsDf = pd.DataFrame.from_dict(retrievedDocsDict,orient='index',columns = ['bm25Score'])\r\n",
    "        \r\n",
    "        ## cleaned golden/vaildation set modules\r\n",
    "            validModules = clean_elective_names(query_val['expectedElectivesInOrder'][queryCount])\r\n",
    "        ## to compute the NDCG of a single query\r\n",
    "            NDCGWithVariousKdf = NDCGWithVariousK(retrievedDocsDf,validModules,0,queryCount)\r\n",
    "        ## if this is 1st NDCG score been compute, make it's df to NDCG df else merge with the current overall NDGC df\r\n",
    "            if queryCount == 0:\r\n",
    "                NDCGDf = NDCGWithVariousKdf\r\n",
    "            else:\r\n",
    "                NDCGDf = pd.merge(NDCGDf, NDCGWithVariousKdf, on=[\"k\"])\r\n",
    "            queryCount += 1\r\n",
    "            \r\n",
    "    ## return a df with all the ndcg results\r\n",
    "    \r\n",
    "    return(NDCGDf)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "## function to compute the NDCG for bm25basic for model 4\r\n",
    "## model 4 BM25 Basic (without query expansion, course information)\r\n",
    "def get_NDCG_BM25BasicNoExpan(query_val,tf,tf_norm,idf):\r\n",
    "    import time\r\n",
    "    import basic_bm25\r\n",
    "    import utils.query_processing\r\n",
    "    \r\n",
    "    ## vairables required for basic_bm25 function\r\n",
    "    vocab = tf.index.tolist()\r\n",
    "    total_length = tf.to_numpy().sum()\r\n",
    "    avg_doc_len = total_length / len(tf.columns) # average document length across all courses\r\n",
    "    \r\n",
    "    ## this section help to compute and obtain the NDCG for each query and store in df\r\n",
    "    import pandas as pd\r\n",
    "    queryCount = 0\r\n",
    "    NDCGDf = 0\r\n",
    "    totalTime = 0\r\n",
    "    for index,row in query_val.iterrows():\r\n",
    "        query = row['querySample']\r\n",
    "        query = utils.query_processing.process_query(query)\r\n",
    "    ## compute basic_bm25 score\r\n",
    "        start = time.time()\r\n",
    "        retrievedDocs, rankedLs = basic_bm25.get_result(query,tf,tf_norm,idf,vocab,avg_doc_len)\r\n",
    "        end =  time.time()- start\r\n",
    "        totalTime += end\r\n",
    "    ## converting moduleNScore to dataframe    \r\n",
    "        retrievedDf = pd.DataFrame.from_dict(retrievedDocs,orient='index',columns = ['bm25Score'])        \r\n",
    "    ## cleaned golden/vaildation set modules\r\n",
    "        validModules = clean_elective_names(query_val['expectedElectivesInOrder'][queryCount])\r\n",
    "    ## to compute the NDCG of a single query\r\n",
    "        NDCGWithVariousKdf = NDCGWithVariousK(retrievedDf,validModules,0,queryCount)\r\n",
    "        \r\n",
    "    ## if this is 1st NDCG score been compute, make it's df to NDCG df else merge with the current overall NDGC df\r\n",
    "        if queryCount == 0:\r\n",
    "            NDCGDf = NDCGWithVariousKdf\r\n",
    "        else:\r\n",
    "            NDCGDf = pd.merge(NDCGDf, NDCGWithVariousKdf, on=[\"k\"])\r\n",
    "        queryCount += 1\r\n",
    "        \r\n",
    "        ## print only when the last query is computed\r\n",
    "        if queryCount == (len(query_val)):\r\n",
    "            averageQueryTime = totalTime/queryCount\r\n",
    "            print('Average Time for {} number of queries : {}'.format(queryCount,averageQueryTime))\r\n",
    "    ## return a df with all the ndcg results\r\n",
    "    \r\n",
    "    return(NDCGDf)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "## function to compute the NDCG for bm25basic for model 5 and 6\r\n",
    "## model 5 BM25 Basic (query expansion, course information)\r\n",
    "## model 6 BM25 Basic (query expansion, course information + survey (50%))\r\n",
    "## model 7 Bm25 with Reformulation (query expansion, course information + survey (50%))\r\n",
    "\r\n",
    "def get_NDCG_BM25Basic(query_val,tf,tf_norm,idf):\r\n",
    "    import basic_bm25\r\n",
    "    import utils.query_processing\r\n",
    "    import time\r\n",
    "    \r\n",
    "    ## vairables required for basic_bm25 function\r\n",
    "    vocab = tf.index.tolist()\r\n",
    "    total_length = tf.to_numpy().sum()\r\n",
    "    avg_doc_len = total_length / len(tf.columns) # average document length across all courses\r\n",
    "    \r\n",
    "    ## this section help to compute and obtain the NDCG for each query and store in df\r\n",
    "    import pandas as pd\r\n",
    "    queryCount = 0\r\n",
    "    NDCGDf = 0\r\n",
    "    totalTime = 0\r\n",
    "    for index,row in query_val.iterrows():\r\n",
    "        query = row['querySample']\r\n",
    "        query = utils.query_processing.process_query(query)\r\n",
    "        \r\n",
    "    ## for query expansion\r\n",
    "        glove_kv = '../pretrained_corpus/glove_6B_300d.kv'   # pretrained vectors for query expansion\r\n",
    "        topn = 3\r\n",
    "        query = utils.query_processing.expand_query(query,glove_kv,topn)\r\n",
    "        \r\n",
    "    ## compute basic_bm25 score and start the timer for querying\r\n",
    "        start = time.time()\r\n",
    "        retrievedDocs, rankedLs = basic_bm25.get_result(query,tf,tf_norm,idf,vocab,avg_doc_len)\r\n",
    "        end =  time.time()- start\r\n",
    "        totalTime += end\r\n",
    "    ## converting moduleNScore to dataframe    \r\n",
    "        retrievedDf = pd.DataFrame.from_dict(retrievedDocs,orient='index',columns = ['bm25Score'])        \r\n",
    "    ## cleaned golden/vaildation set modules\r\n",
    "        validModules = clean_elective_names(query_val['expectedElectivesInOrder'][queryCount])\r\n",
    "    ## to compute the NDCG of a single query\r\n",
    "        NDCGWithVariousKdf = NDCGWithVariousK(retrievedDf,validModules,0,queryCount)\r\n",
    "        \r\n",
    "    ## if this is 1st NDCG score been compute, make it's df to NDCG df else merge with the current overall NDGC df\r\n",
    "        if queryCount == 0:\r\n",
    "            NDCGDf = NDCGWithVariousKdf\r\n",
    "        else:\r\n",
    "            NDCGDf = pd.merge(NDCGDf, NDCGWithVariousKdf, on=[\"k\"])\r\n",
    "        queryCount += 1\r\n",
    "        \r\n",
    "    ## print only when the last query is computed\r\n",
    "        if queryCount == (len(query_val)):\r\n",
    "            averageQueryTime = totalTime/queryCount\r\n",
    "            print('Average Time for {} number of queries : {}'.format(queryCount,averageQueryTime))\r\n",
    "    ## return a df with all the ndcg results\r\n",
    "    \r\n",
    "    return(NDCGDf)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "## function to compute the NDCG for bm25basic for model 8\r\n",
    "## model 8 Bm25 with Reformulation and Pseudo Relevance Feedback (query expansion, course information + survey (50%))\r\n",
    "def get_NDCG_BM25WPseudo(query_val,tf,tf_norm,idf):\r\n",
    "    import bm25_with_pseudo_relevance\r\n",
    "    import utils.query_processing\r\n",
    "    import time\r\n",
    "    import pandas as pd\r\n",
    "    ## vairables required for basic_bm25 function\r\n",
    "    vocab = tf.index.tolist()\r\n",
    "    total_length = tf.to_numpy().sum()\r\n",
    "    avg_doc_len = total_length / len(tf.columns) # average document length across all courses\r\n",
    "    norm_association_matrix = pd.read_csv('../data/trained_scores/norm_association_matrix_trained.csv', header = 0, index_col = 0)\r\n",
    "    df = pd.read_csv('../data/trained_scores/course_info_with_survey_df_trained.csv', header=0, index_col=0)\r\n",
    "    \r\n",
    "    ## this section help to compute and obtain the NDCG for each query and store in df\r\n",
    "    queryCount = 0\r\n",
    "    NDCGDf = 0\r\n",
    "    totalTime = 0\r\n",
    "    for index,row in query_val.iterrows():\r\n",
    "        query = row['querySample']\r\n",
    "    ## do not run glove_kv, topn, expand_query for  get_NDCG_BM25WPseudo\r\n",
    "        \r\n",
    "    ## compute basic_bm25 score and start the timer for querying\r\n",
    "        start = time.time()\r\n",
    "        retrievedDocs, rankedLs = bm25_with_pseudo_relevance.bm25_pseudo_relevance_back(query, df, tf, tf_norm, idf, norm_association_matrix, vocab, avg_doc_len, k=10)\r\n",
    "        end =  time.time()- start\r\n",
    "        totalTime += end\r\n",
    "    ## converting moduleNScore to dataframe    \r\n",
    "        retrievedDf = pd.DataFrame.from_dict(retrievedDocs,orient='index',columns = ['bm25Score'])        \r\n",
    "    ## cleaned golden/vaildation set modules\r\n",
    "        validModules = clean_elective_names(query_val['expectedElectivesInOrder'][queryCount])\r\n",
    "    ## to compute the NDCG of a single query\r\n",
    "        NDCGWithVariousKdf = NDCGWithVariousK(retrievedDf,validModules,0,queryCount)\r\n",
    "        \r\n",
    "    ## if this is 1st NDCG score been compute, make it's df to NDCG df else merge with the current overall NDGC df\r\n",
    "        if queryCount == 0:\r\n",
    "            NDCGDf = NDCGWithVariousKdf\r\n",
    "        else:\r\n",
    "            NDCGDf = pd.merge(NDCGDf, NDCGWithVariousKdf, on=[\"k\"])\r\n",
    "        queryCount += 1\r\n",
    "        \r\n",
    "    ## print only when the last query is computed\r\n",
    "        if queryCount == (len(query_val)):\r\n",
    "            averageQueryTime = totalTime/queryCount\r\n",
    "            print('Average Time for {} number of queries : {}'.format(queryCount,averageQueryTime))\r\n",
    "    ## return a df with all the ndcg results\r\n",
    "    \r\n",
    "    return(NDCGDf)\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "if __name__ == \"__main__\":\r\n",
    "    if False:\r\n",
    "        ## for Cosine Similarity (without and with query expansion, course information + (50% survey))\r\n",
    "        import pandas as pd\r\n",
    "        tf = pd.read_csv('../data/course_info_scores/course_info_tf.csv', index_col = 0)\r\n",
    "        query_val= pd.read_csv('../data/survey/vaildation_sample_query.csv',index_col = 0)\r\n",
    "        model1NDCG = get_NDCG_cosine_no_expan(query_val,tf)\r\n",
    "        model1NDCGAverage = model1NDCG.iloc[:, 1:].mean(axis=1)\r\n",
    "        # model1NDCGAverage.to_csv('../results/ndcg_score/ndcg_score_mdoel1.csv')\r\n",
    "\r\n",
    "        tf = pd.read_csv('../data/course_info_scores/course_info_tf.csv', index_col = 0)\r\n",
    "        query_val= pd.read_csv('../data/survey/vaildation_sample_query.csv',index_col = 0)\r\n",
    "        model2NDCG = get_NDCG_cosine(query_val,tf)\r\n",
    "        model2NDCGAverage = model2NDCG.iloc[:, 1:].mean(axis=1)\r\n",
    "        # model2NDCGAverage.to_csv('../results/ndcg_score/ndcg_score_mdoel2.csv')\r\n",
    "\r\n",
    "        tf = pd.read_csv('../data/course_info_with_survey_scores/course_info_with_survey_tf.csv', index_col = 0)\r\n",
    "        query_val= pd.read_csv('../data/survey/vaildation_sample_query.csv',index_col = 0)\r\n",
    "        model3NDCG = get_NDCG_cosine(query_val,tf)\r\n",
    "        model3NDCGAverage = model3NDCG.iloc[:, :].mean(axis=1)\r\n",
    "        # model3NDCGAverage.to_csv('../results/ndcg_score/ndcg_score_mdoel3.csv')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "if __name__ == \"__main__\":\r\n",
    "    if False:\r\n",
    "        import pandas as pdf\r\n",
    "        ## to create the function to obtain the NDCG for model 4 to 6\r\n",
    "        ## model 4 BM25 Basic (without query expansion, course information)\r\n",
    "        query_val= pd.read_csv('../data/survey/vaildation_sample_query.csv',index_col = 0)\r\n",
    "        tf = pd.read_csv('../data/course_info_scores/course_info_tf.csv', index_col = 0)\r\n",
    "        tf_norm = pd.read_csv('../data/course_info_scores/course_info_tf_norm.csv', index_col = 0)\r\n",
    "        idf = pd.read_csv('../data/course_info_scores/course_info_idf.csv', header=0, index_col=0)\r\n",
    "        model4NDCG = get_NDCG_BM25BasicNoExpan(query_val,tf,tf_norm,idf)\r\n",
    "        model4NDCGAverage = model4NDCG.iloc[:, 1:].mean(axis=1)\r\n",
    "        # model4NDCGAverage.to_csv('../results/ndcg_score/ndcg_score_mdoel4.csv')\r\n",
    "        print('\\n')\r\n",
    "\r\n",
    "        \r\n",
    "        ## model 5 BM25 Basic (query expansion, course information)\r\n",
    "        query_val= pd.read_csv('../data/survey/vaildation_sample_query.csv',index_col = 0)\r\n",
    "        tf = pd.read_csv('../data/course_info_scores/course_info_tf.csv', index_col = 0)\r\n",
    "        tf_norm = pd.read_csv('../data/course_info_scores/course_info_tf_norm.csv', index_col = 0)\r\n",
    "        idf = pd.read_csv('../data/course_info_scores/course_info_idf.csv', header=0, index_col=0)\r\n",
    "        model5NDCG = get_NDCG_BM25Basic(query_val,tf,tf_norm,idf)\r\n",
    "        model5NDCGAverage = model5NDCG.iloc[:, 1:].mean(axis=1)\r\n",
    "        # model5NDCGAverage.to_csv('../results/ndcg_score/ndcg_score_mdoel5.csv')\r\n",
    "        print('\\n')\r\n",
    "        \r\n",
    "        ## model 6 BM25 Basic (query expansion, course information)\r\n",
    "        ##BM25 Basic (query expansion, course information + survey (50%))\r\n",
    "        query_val= pd.read_csv('../data/survey/vaildation_sample_query.csv',index_col = 0)\r\n",
    "        tf = pd.read_csv('../data/course_info_with_survey_scores/course_info_with_survey_tf.csv', index_col = 0)\r\n",
    "        tf_norm = pd.read_csv('../data/course_info_with_survey_scores/course_info_with_survey_tf_norm.csv', index_col = 0)\r\n",
    "        idf = pd.read_csv('../data/course_info_with_survey_scores/course_info_with_survey_idf.csv', header=0, index_col=0)\r\n",
    "        model6NDCG = get_NDCG_BM25Basic(query_val,tf,tf_norm,idf)\r\n",
    "        model6NDCGAverage = model6NDCG.iloc[:, 1:].mean(axis=1)\r\n",
    "        # model6NDCGAverage.to_csv('../results/ndcg_score/ndcg_score_mdoel6.csv')\r\n",
    "        print('\\n')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "if __name__ == \"__main__\":\r\n",
    "    import pandas as pdf\r\n",
    "    if False:\r\n",
    "        ## to create the function to obtain the NDCG for model 7 and 8\r\n",
    "        ## model 7 Bm25 with Reformulation (query expansion, course information + survey (50%))\r\n",
    "        query_val= pd.read_csv('../data/survey/vaildation_sample_query.csv',index_col = 0)\r\n",
    "        tf = pd.read_csv('../data/trained_scores/course_info_with_survey_tf_trained.csv', index_col = 0)\r\n",
    "        tf_norm = pd.read_csv('../data/trained_scores/course_info_with_survey_tf_norm_trained.csv', index_col = 0)\r\n",
    "        idf = pd.read_csv('../data/trained_scores/course_info_with_survey_idf_trained.csv', header=0, index_col=0)\r\n",
    "        model7NDCG = get_NDCG_BM25Basic(query_val,tf,tf_norm,idf)\r\n",
    "        model7NDCGAverage = model7NDCG.iloc[:, 1:].mean(axis=1)\r\n",
    "        # model7NDCGAverage.to_csv('../results/ndcg_score/ndcg_score_mdoel7.csv')\r\n",
    "        print('\\n')\r\n",
    "        \r\n",
    "        ## model 8 Bm25 with Reformulation and Pseudo Relevance Feedback (query expansion, course information + survey (50%))\r\n",
    "        query_val= pd.read_csv('../data/survey/vaildation_sample_query.csv',index_col = 0)\r\n",
    "        tf = pd.read_csv('../data/trained_scores/course_info_with_survey_tf_trained.csv', index_col = 0)\r\n",
    "        tf_norm = pd.read_csv('../data/trained_scores/course_info_with_survey_tf_norm_trained.csv', index_col = 0)\r\n",
    "        idf = pd.read_csv('../data/trained_scores/course_info_with_survey_idf_trained.csv', header=0, index_col=0)\r\n",
    "        model8NDCG = get_NDCG_BM25WPseudo(query_val,tf,tf_norm,idf)    \r\n",
    "        model8NDCGAverage = model8NDCG.iloc[:, 1:].mean(axis=1)\r\n",
    "        # model8NDCGAverage.to_csv('../results/ndcg_score/ndcg_score_mdoel8.csv')\r\n",
    "    print('\\n')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "running basic bm25\n",
      "running bm25 for reformulated query\n",
      "running basic bm25\n",
      "running bm25 for reformulated query\n",
      "running basic bm25\n",
      "running bm25 for reformulated query\n",
      "running basic bm25\n",
      "running bm25 for reformulated query\n",
      "running basic bm25\n",
      "running bm25 for reformulated query\n",
      "running basic bm25\n",
      "running bm25 for reformulated query\n",
      "running basic bm25\n",
      "running bm25 for reformulated query\n",
      "running basic bm25\n",
      "running bm25 for reformulated query\n",
      "running basic bm25\n",
      "running bm25 for reformulated query\n",
      "running basic bm25\n",
      "running bm25 for reformulated query\n",
      "running basic bm25\n",
      "running bm25 for reformulated query\n",
      "running basic bm25\n",
      "running bm25 for reformulated query\n",
      "running basic bm25\n",
      "running bm25 for reformulated query\n",
      "running basic bm25\n",
      "running bm25 for reformulated query\n",
      "running basic bm25\n",
      "running bm25 for reformulated query\n",
      "running basic bm25\n",
      "running bm25 for reformulated query\n",
      "running basic bm25\n",
      "running bm25 for reformulated query\n",
      "running basic bm25\n",
      "running bm25 for reformulated query\n",
      "running basic bm25\n",
      "running bm25 for reformulated query\n",
      "running basic bm25\n",
      "running bm25 for reformulated query\n",
      "running basic bm25\n",
      "running bm25 for reformulated query\n",
      "running basic bm25\n",
      "running bm25 for reformulated query\n",
      "running basic bm25\n",
      "running bm25 for reformulated query\n",
      "running basic bm25\n",
      "running bm25 for reformulated query\n",
      "running basic bm25\n",
      "running bm25 for reformulated query\n",
      "running basic bm25\n",
      "running bm25 for reformulated query\n",
      "running basic bm25\n",
      "running bm25 for reformulated query\n",
      "running basic bm25\n",
      "running bm25 for reformulated query\n",
      "running basic bm25\n",
      "running bm25 for reformulated query\n",
      "running basic bm25\n",
      "running bm25 for reformulated query\n",
      "Average Time for 30 number of queries : 126.65554592609405\n",
      "\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit"
  },
  "interpreter": {
   "hash": "2647ea34e536f865ab67ff9ddee7fd78773d956cec0cab53c79b32cd10da5d83"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}