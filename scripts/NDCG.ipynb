{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "def dcg_at_k(r, k, method=0):\r\n",
    "    \"\"\"Score is discounted cumulative gain (dcg)\r\n",
    "\r\n",
    "    Relevance is positive real values.  Can use binary\r\n",
    "    as the previous methods.\r\n",
    "\r\n",
    "    Example from\r\n",
    "    http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\r\n",
    "    >>> r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\r\n",
    "    >>> dcg_at_k(r, 1)\r\n",
    "    3.0\r\n",
    "    >>> dcg_at_k(r, 1, method=1)\r\n",
    "    3.0\r\n",
    "    >>> dcg_at_k(r, 2)\r\n",
    "    5.0\r\n",
    "    >>> dcg_at_k(r, 2, method=1)\r\n",
    "    4.2618595071429155\r\n",
    "    >>> dcg_at_k(r, 10)\r\n",
    "    9.6051177391888114\r\n",
    "    >>> dcg_at_k(r, 11)\r\n",
    "    9.6051177391888114\r\n",
    "\r\n",
    "    Args:\r\n",
    "        r: Relevance scores (list or numpy) in rank order\r\n",
    "            (first element is the first item)\r\n",
    "        k: Number of results to consider\r\n",
    "        method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]\r\n",
    "\r\n",
    "    Returns:\r\n",
    "        Discounted cumulative gain\r\n",
    "    \"\"\"\r\n",
    "    import numpy as np\r\n",
    "    r = np.asfarray(r)[:k]\r\n",
    "    if r.size:\r\n",
    "        return r[0] + np.sum(r[1:] / np.log2(np.arange(2, r.size + 1)))\r\n",
    "    return 0."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "## from week 6 lab\r\n",
    "def ndcg_at_k(r, k, method=0):\r\n",
    "    \"\"\"Score is normalized discounted cumulative gain (ndcg)\r\n",
    "\r\n",
    "    Relevance is positive real values.  Can use binary\r\n",
    "    as the previous methods.\r\n",
    "\r\n",
    "    Example from\r\n",
    "    http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\r\n",
    "    >>> r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\r\n",
    "    >>> ndcg_at_k(r, 1)\r\n",
    "    1.0\r\n",
    "    >>> r = [2, 1, 2, 0]\r\n",
    "    >>> ndcg_at_k(r, 4)\r\n",
    "    0.9203032077642922\r\n",
    "    >>> ndcg_at_k(r, 4, method=1)\r\n",
    "    0.96519546960144276\r\n",
    "    >>> ndcg_at_k([0], 1)\r\n",
    "    0.0\r\n",
    "    >>> ndcg_at_k([1], 2)\r\n",
    "    1.0\r\n",
    "\r\n",
    "    Args:\r\n",
    "        r: Relevance scores (list or numpy) in rank order\r\n",
    "            (first element is the first item)\r\n",
    "        k: Number of results to consider\r\n",
    "        method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]\r\n",
    "                If 1 then weights are [1.0, 0.6309, 0.5, 0.4307, ...]\r\n",
    "\r\n",
    "    Returns:\r\n",
    "        Normalized discounted cumulative gain\r\n",
    "    \"\"\"\r\n",
    "    import numpy as np\r\n",
    "\r\n",
    "    dcg_max = dcg_at_k(sorted(r, reverse=True), k, method)\r\n",
    "    if not dcg_max:\r\n",
    "        return 0.\r\n",
    "    return dcg_at_k(r, k, method) / dcg_max"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "def assigningBM25ScoreToRelevantAndRetrieved(_bm25ScoreDf, relevantAndRetrievedDocs):\r\n",
    "    df = _bm25ScoreDf.copy(deep = False)\r\n",
    "    irrelevantAndRetrievedDocsList = list(set(df.index) - set(relevantAndRetrievedDocs))\r\n",
    "    \r\n",
    "    for relevantAndRetrievedDoc in irrelevantAndRetrievedDocsList:\r\n",
    "        df.loc[relevantAndRetrievedDoc]['bm25Score'] = 0\r\n",
    "    return(df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "## based setup\r\n",
    "## assume docs are in order\r\n",
    "retrievedDocs = ['D','C', 'B','A'] \r\n",
    "retrievedDocsScore = [0.43, 0.26, 0.03, 0.37]\r\n",
    "\r\n",
    "relevantAndRetrievedDocs = ['B','D']\r\n",
    "\r\n",
    "retrievedDocsDict = {}\r\n",
    "for index in range(len(retrievedDocs)):\r\n",
    "    retrievedDocsDict[retrievedDocs[index]] = retrievedDocsScore[index]\r\n",
    "import pandas as pd\r\n",
    "retrievedDocsDf = pd.DataFrame.from_dict(retrievedDocsDict,orient='index',columns = ['bm25Score'])\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "BM25ScoreToRelevantAndRetrieved = assigningBM25ScoreToRelevantAndRetrieved(retrievedDocsDf,relevantAndRetrievedDocs)\r\n",
    "BM25ScoreToRelevantAndRetrieved"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   bm25Score\n",
       "D       0.43\n",
       "C       0.00\n",
       "B       0.03\n",
       "A       0.00"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bm25Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>D</th>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "BM25ScoreToRelevantAndRetrievedScoreList = list(BM25ScoreToRelevantAndRetrieved.bm25Score)\r\n",
    "BM25ScoreToRelevantAndRetrievedScoreList"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.43, 0.0, 0.03, 0.0]"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "\"\"\"[summary]\r\n",
    "There are still difference between the lab ndcg and mannual caulcation NDCG\r\n",
    "\"\"\"\r\n",
    "ndcg_at_kScore = ndcg_at_k(BM25ScoreToRelevantAndRetrievedScoreList,4)\r\n",
    "ndcg_at_kScore\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9759302013198777"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import pandas as pd\r\n",
    "scrapped = pd.read_csv('../data/bm25/bm25_no_survey/merged_courses_tf.csv',index_col = 0)\r\n",
    "survey = pd.read_csv('../data/survey_processing/surveyToBeTokenized.csv',index_col = 0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "wordCounter = 0\r\n",
    "listOfWordInScrappedNSurvey = []\r\n",
    "for suveryWord in survey.index:\r\n",
    "    if suveryWord in scrapped.index:\r\n",
    "        listOfWordInScrappedNSurvey.append(suveryWord)\r\n",
    "        wordCounter +=1\r\n",
    "wordCounter"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "SampleQuery1 = 'computational, analysis, solidity, mongodb, evaluation'\r\n",
    "SampleQuery1 = SampleQuery1.split(\", \")\r\n",
    "\r\n",
    "SampleQuery2 = 'analysis, ampl, cast, sklearn, ui'\r\n",
    "SampleQuery2 = SampleQuery2.split(\", \")\r\n",
    "\r\n",
    "\r\n",
    "SampleQuery3 = 'real, soup, social, computational, tensorflow'\r\n",
    "SampleQuery3 = SampleQuery3.split(\", \")\r\n",
    "\r\n",
    "SampleQuery4 = 'cleaning, business, real, soup, concept'\r\n",
    "SampleQuery4 = SampleQuery4.split(\", \")\r\n",
    "\r\n",
    "SampleQuery5 = 'wireframing, inventory, long, system, technology'\r\n",
    "SampleQuery5 = SampleQuery5.split(\", \")\r\n",
    "\r\n",
    "listOfSampleQuery = [SampleQuery1, SampleQuery2, SampleQuery3, SampleQuery4, SampleQuery5]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "for sampleQuery in listOfSampleQuery:\r\n",
    "    wordCounter = 0\r\n",
    "    listOfWordInScrappedNSurvey = []\r\n",
    "    for word in sampleQuery:\r\n",
    "        if word in scrapped.index:\r\n",
    "            listOfWordInScrappedNSurvey.append(word)\r\n",
    "            wordCounter +=1\r\n",
    "    print('Common Words between scrapped and SampleQuery{}: {}'.format(sampleQuery,wordCounter))\r\n",
    "    print('Common terms : {}\\n'.format(listOfWordInScrappedNSurvey))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Common Words between scrapped and SampleQuery['computational', 'analysis', 'solidity', 'mongodb', 'evaluation']: 3\n",
      "Common terms : ['computational', 'analysis', 'evaluation']\n",
      "\n",
      "Common Words between scrapped and SampleQuery['analysis', 'ampl', 'cast', 'sklearn', 'ui']: 3\n",
      "Common terms : ['analysis', 'cast', 'sklearn']\n",
      "\n",
      "Common Words between scrapped and SampleQuery['real', 'soup', 'social', 'computational', 'tensorflow']: 4\n",
      "Common terms : ['real', 'social', 'computational', 'tensorflow']\n",
      "\n",
      "Common Words between scrapped and SampleQuery['cleaning', 'business', 'real', 'soup', 'concept']: 3\n",
      "Common terms : ['business', 'real', 'concept']\n",
      "\n",
      "Common Words between scrapped and SampleQuery['wireframing', 'inventory', 'long', 'system', 'technology']: 4\n",
      "Common terms : ['inventory', 'long', 'system', 'technology']\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit"
  },
  "interpreter": {
   "hash": "2647ea34e536f865ab67ff9ddee7fd78773d956cec0cab53c79b32cd10da5d83"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}