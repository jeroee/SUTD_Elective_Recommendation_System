{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "## from week 6 lab\r\n",
    "def dcg_at_k(r, k, method=0):\r\n",
    "    \"\"\"Score is discounted cumulative gain (dcg)\r\n",
    "\r\n",
    "    Relevance is positive real values.  Can use binary\r\n",
    "    as the previous methods.\r\n",
    "\r\n",
    "    Example from\r\n",
    "    http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\r\n",
    "    >>> r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\r\n",
    "    >>> dcg_at_k(r, 1)\r\n",
    "    3.0\r\n",
    "    >>> dcg_at_k(r, 1, method=1)\r\n",
    "    3.0\r\n",
    "    >>> dcg_at_k(r, 2)\r\n",
    "    5.0\r\n",
    "    >>> dcg_at_k(r, 2, method=1)\r\n",
    "    4.2618595071429155\r\n",
    "    >>> dcg_at_k(r, 10)\r\n",
    "    9.6051177391888114\r\n",
    "    >>> dcg_at_k(r, 11)\r\n",
    "    9.6051177391888114\r\n",
    "\r\n",
    "    Args:\r\n",
    "        r: Relevance scores (list or numpy) in rank order\r\n",
    "            (first element is the first item)\r\n",
    "        k: Number of results to consider\r\n",
    "        method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]\r\n",
    "\r\n",
    "    Returns:\r\n",
    "        Discounted cumulative gain\r\n",
    "    \"\"\"\r\n",
    "    import numpy as np\r\n",
    "    r = np.asfarray(r)[:k]\r\n",
    "    if r.size: ## why is this r.size? when will this be false?\r\n",
    "        return r[0] + np.sum(r[1:] / np.log2(np.arange(2, r.size + 1)))\r\n",
    "    return 0."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "## from week 6 lab\r\n",
    "def ndcg_at_k(r, k, method=0):\r\n",
    "    \"\"\"Score is normalized discounted cumulative gain (ndcg)\r\n",
    "\r\n",
    "    Relevance is positive real values.  Can use binary\r\n",
    "    as the previous methods.\r\n",
    "\r\n",
    "    Example from\r\n",
    "    http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\r\n",
    "    >>> r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\r\n",
    "    >>> ndcg_at_k(r, 1)\r\n",
    "    1.0\r\n",
    "    >>> r = [2, 1, 2, 0]\r\n",
    "    >>> ndcg_at_k(r, 4)\r\n",
    "    0.9203032077642922\r\n",
    "    >>> ndcg_at_k(r, 4, method=1)\r\n",
    "    0.96519546960144276\r\n",
    "    >>> ndcg_at_k([0], 1)\r\n",
    "    0.0\r\n",
    "    >>> ndcg_at_k([1], 2)\r\n",
    "    1.0\r\n",
    "\r\n",
    "    Args:\r\n",
    "        r: Relevance scores (list or numpy) in rank order\r\n",
    "            (first element is the first item)\r\n",
    "        k: Number of results to consider\r\n",
    "        method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]\r\n",
    "                If 1 then weights are [1.0, 0.6309, 0.5, 0.4307, ...]\r\n",
    "\r\n",
    "    Returns:\r\n",
    "        Normalized discounted cumulative gain\r\n",
    "    \"\"\"\r\n",
    "    import numpy as np\r\n",
    "\r\n",
    "    dcg_max = dcg_at_k(sorted(r, reverse=True), k, method)\r\n",
    "    if not dcg_max:\r\n",
    "        return 0.\r\n",
    "    print('For k is {}, DCG scorce is {}'.format(k,dcg_at_k(r, k, method)))\r\n",
    "    print('For k is {}, IDCG scorce is {}'.format(k,dcg_max))\r\n",
    "    return dcg_at_k(r, k, method) / dcg_max"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def assigningBM25ScoreToRelevantAndRetrieved(_bm25ScoreDf, relevantDocsList):\r\n",
    "    \"\"\"[summary]\r\n",
    "    This function helps to assign zero values to those non-relevant and retrieved documents\r\n",
    "    It retents the score of those relevant and retrieved\r\n",
    "\r\n",
    "    Args:\r\n",
    "        _bm25ScoreDf ([dataframe]): [a dataframe where rows are modules and columns is the bm25 scores]\r\n",
    "        relevantAndRetrievedDocs ([list]): [list of modules based on the golden standard(idea outcome based on survey)]\r\n",
    "    \"\"\"\r\n",
    "    df = _bm25ScoreDf.copy(deep = False)\r\n",
    "    irrelevantAndRetrievedDocsList = list(set(df.index) - set(relevantDocsList))\r\n",
    "    \r\n",
    "    for relevantAndRetrievedDoc in irrelevantAndRetrievedDocsList:\r\n",
    "        df.loc[relevantAndRetrievedDoc]['bm25Score'] = 0\r\n",
    "    \"\"\"[summary]\r\n",
    "    output is a df with score that are retrieved and relevant(relevant depends on the gold standard)\r\n",
    "    \"\"\"\r\n",
    "    return(df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def NDCGWithVariousK(retrievedDocsDf,listOfRelevantDocs, exportResults = 0, queryNum = '', fileName = 'test'):\r\n",
    "    \"\"\"[summary]\r\n",
    "    This function compute the NDGC at vaious K\r\n",
    "\r\n",
    "    Args:\r\n",
    "        retrievedDocsDf ([dataframe]): [dataframe of retrieved documents and it's bm25 score]\r\n",
    "        listOfRelevantDocs ([list]): [list of relevant Documents based on gold standard]\r\n",
    "        exportResults (int, optional): [to determine to export ndcg results]. Defaults to 0 and 1 to export ndgc score\r\n",
    "        fileName (str, optional): [fileName to be exported ideally it should be the \"ndcg_score_'model name']. Defaults to 'test'.\r\n",
    "    \"\"\"\r\n",
    "    ## assign zero values to those non-relevant and retrieved documents, It retain the score of those relevant and retrieved\r\n",
    "    BM25ScoreToRelevantAndRetrieved = assigningBM25ScoreToRelevantAndRetrieved(retrievedDocsDf,listOfRelevantDocs)\r\n",
    "    ## obtain the score of the BM25 of the relevant and retrieved modules\r\n",
    "    BM25ScoreToRelevantAndRetrievedScoreList = list(BM25ScoreToRelevantAndRetrieved.bm25Score)\r\n",
    "    \r\n",
    "    ## dict to save NDCGScore ie {k(ranking):NDCG Score}\r\n",
    "    NDCGScoreDict = {}\r\n",
    "    for i in range(1,len(BM25ScoreToRelevantAndRetrievedScoreList)):\r\n",
    "        ndcg_at_kScore = ndcg_at_k(BM25ScoreToRelevantAndRetrievedScoreList,i)\r\n",
    "        print('For k is {}, NDCG scorce is {}\\n'.format(i,ndcg_at_kScore))\r\n",
    "        NDCGScoreDict[i] = ndcg_at_kScore\r\n",
    "    \r\n",
    "    ## convert dict to df for easier sorting analysis of the scores and exporting it to csv\r\n",
    "    import pandas as pd\r\n",
    "    NDCGDf = pd.DataFrame.from_dict(NDCGScoreDict,orient='index',columns=['NDCGScore{}'.format(queryNum)])\r\n",
    "    NDCGDf.reset_index(inplace = True)\r\n",
    "    ## rename the column to k columns \r\n",
    "    NDCGDf.rename(columns={\"index\": \"k\"}, inplace = True)\r\n",
    "    \r\n",
    "    ## to export the ndcg scores to csv if exportResults == 1\r\n",
    "    if exportResults == 1:\r\n",
    "        fileName = 'ndcg_score_{}.csv'.format(fileName)\r\n",
    "        NDCGDf.to_csv('../results/ndcg_score/{}'.format(fileName))\r\n",
    "    return(NDCGDf)\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Toy Problem formulation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "\"Test Case : the retrievedDocScore\"\r\n",
    "## assume docs are not in bm25 scorce order\r\n",
    "retrievedDocs = ['D','C', 'B','A'] \r\n",
    "retrievedDocsScore = [0.43, 0.26, 0.03, 0.37]\r\n",
    "## I realised that the score should be in ascending order of bm25 score hence I made some changes to fit our use case\r\n",
    "# retrievedDocsScore = [0.43,  0.37, 0.26, 0.03]\r\n",
    "\r\n",
    "## creating a retrievedDocsDf for test cases\r\n",
    "## this should be the same format of the bm25 output\r\n",
    "retrievedDocsDict = {}\r\n",
    "for index in range(len(retrievedDocs)):\r\n",
    "    retrievedDocsDict[retrievedDocs[index]] = retrievedDocsScore[index]\r\n",
    "import pandas as pd\r\n",
    "retrievedDocsDf1 = pd.DataFrame.from_dict(retrievedDocsDict,orient='index',columns = ['bm25Score'])\r\n",
    "\r\n",
    "print('BM25 output:')\r\n",
    "retrievedDocsDf1"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "BM25 output:\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   bm25Score\n",
       "D       0.43\n",
       "C       0.26\n",
       "B       0.03\n",
       "A       0.37"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bm25Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>D</th>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "\"Test Case : the retrievedDocScore\"\r\n",
    "## assume docs are not in bm25 scorce order\r\n",
    "retrievedDocs = ['C','D', 'B','A'] \r\n",
    "retrievedDocsScore = [0.5, 0.3, 0.2, 0.1]\r\n",
    "## I realised that the score should be in ascending order of bm25 score hence I made some changes to fit our use case\r\n",
    "# retrievedDocsScore = [0.43,  0.37, 0.26, 0.03]\r\n",
    "\r\n",
    "## creating a retrievedDocsDf for test cases\r\n",
    "## this should be the same format of the bm25 output\r\n",
    "retrievedDocsDict = {}\r\n",
    "for index in range(len(retrievedDocs)):\r\n",
    "    retrievedDocsDict[retrievedDocs[index]] = retrievedDocsScore[index]\r\n",
    "import pandas as pd\r\n",
    "retrievedDocsDf2 = pd.DataFrame.from_dict(retrievedDocsDict,orient='index',columns = ['bm25Score'])\r\n",
    "\r\n",
    "print('BM25 output:')\r\n",
    "retrievedDocsDf2"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "BM25 output:\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   bm25Score\n",
       "C        0.5\n",
       "D        0.3\n",
       "B        0.2\n",
       "A        0.1"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bm25Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D</th>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "\"Test Case : The Relevant Docs\"\r\n",
    "relevantDocs1 = ['B','D','E']\r\n",
    "print('List of relevant Docs: {}'.format(relevantDocs1))\r\n",
    "relevantDocs2 = ['A','C']\r\n",
    "print('List of relevant Docs: {}'.format(relevantDocs2))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "List of relevant Docs: ['B', 'D', 'E']\n",
      "List of relevant Docs: ['A', 'C']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "retrievedlist = [retrievedDocsDf1,retrievedDocsDf2]\r\n",
    "relevantlist =[relevantDocs1,relevantDocs2]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "if __name__ == \"__main__\":\r\n",
    "    import pandas as pd\r\n",
    "    ## this index is meant to keep track of the NDCG score of each query\r\n",
    "    queryIndex = 0\r\n",
    "    for retrieved in retrievedlist:\r\n",
    "    ## to compute the NDCG of a single query\r\n",
    "        NDCGWithVariousKdf = NDCGWithVariousK(retrieved,relevantlist[queryIndex],0,queryIndex)\r\n",
    "    ## if this is 1st NDCG score been compute, make it's df to NDCG df else merge with the current overall NDGC df\r\n",
    "        if queryIndex == 0:\r\n",
    "            NDCGDf = NDCGWithVariousKdf\r\n",
    "        else:\r\n",
    "            NDCGDf = pd.merge(NDCGDf, NDCGWithVariousKdf, on=[\"k\"])\r\n",
    "        queryIndex += 1"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "For k is 1, DCG scorce is 0.43\n",
      "For k is 1, IDCG scorce is 0.43\n",
      "For k is 1, NDCG scorce is 1.0\n",
      "\n",
      "For k is 2, DCG scorce is 0.43\n",
      "For k is 2, IDCG scorce is 0.45999999999999996\n",
      "For k is 2, NDCG scorce is 0.9347826086956522\n",
      "\n",
      "For k is 3, DCG scorce is 0.4489278926071437\n",
      "For k is 3, IDCG scorce is 0.45999999999999996\n",
      "For k is 3, NDCG scorce is 0.9759302013198777\n",
      "\n",
      "For k is 1, DCG scorce is 0.5\n",
      "For k is 1, IDCG scorce is 0.5\n",
      "For k is 1, NDCG scorce is 1.0\n",
      "\n",
      "For k is 2, DCG scorce is 0.5\n",
      "For k is 2, IDCG scorce is 0.6\n",
      "For k is 2, NDCG scorce is 0.8333333333333334\n",
      "\n",
      "For k is 3, DCG scorce is 0.5\n",
      "For k is 3, IDCG scorce is 0.6\n",
      "For k is 3, NDCG scorce is 0.8333333333333334\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "import pandas as pd \r\n",
    "import numpy as np\r\n",
    "from gensim.models import KeyedVectors\r\n",
    "from gensim.parsing.preprocessing import remove_stopwords\r\n",
    "import nltk\r\n",
    "from nltk.stem import WordNetLemmatizer\r\n",
    "from nltk.corpus import wordnet\r\n",
    "import operator\r\n",
    "import time\r\n",
    "\r\n",
    "# import logging\r\n",
    "# logging.basicConfig(level=logging.INFO, format='%(message)s')\r\n",
    "# logger = logging.getLogger()\r\n",
    "# logger.addHandler(logging.FileHandler(\"logs/MAP.log\", 'a'))\r\n",
    "# print = logger.info\r\n",
    "\r\n",
    "\r\n",
    "from utils.association_matrix import  get_top_k_associated_words, get_associated_words\r\n",
    "from utils.query_processing import get_wordnet_pos, process_query, expand_query\r\n",
    "from basic_bm25 import bm25_basic, get_result\r\n",
    "from bm25_with_pseudo_relevance import bm25_pseudo_relevance_back\r\n",
    "nltk.download('punkt')\r\n",
    "nltk.download('wordnet')\r\n",
    "nltk.download('averaged_perceptron_tagger')\r\n",
    "lemmatizer = WordNetLemmatizer()\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_NDCG_pseudo_relevance(query_val, tf=tf, tf_norm=tf_norm, idf=idf):\r\n",
    "    top_retrieved = 10\r\n",
    "    rs = []\r\n",
    "    for index, row in query_val.iterrows():\r\n",
    "\r\n",
    "        vocab = tf.index.tolist()  # unique words\r\n",
    "        total_length = tf.to_numpy().sum()\r\n",
    "        avg_doc_len = total_length / len(tf.columns) # average document length across all courses\r\n",
    "        query_original = row['querySample']  # take in query from training sample\r\n",
    "        #query = process_query(query=query_original)\r\n",
    "\r\n",
    "        result, ls = bm25_pseudo_relevance_back(query=query_original, tf=tf, tf_norm=tf_norm, idf=idf, vocab=vocab, avg_doc_len=avg_doc_len, k=10)\r\n",
    "        predicted = ls[:top_retrieved] # retrieve top 10 courses from predictions\r\n",
    "        relevant_results = eval(row['expectedElectivesInOrder'])\r\n",
    "        relevant_results = clean_elective_names(relevant_results)  \r\n",
    "        \r\n",
    "        #print('rpedicted: ', predicted)\r\n",
    "        #print('relevant: ', relevant_results)\r\n",
    "        r = []\r\n",
    "        for query_result in predicted:\r\n",
    "            if query_result in relevant_results:\r\n",
    "                r.append(1)\r\n",
    "            else:\r\n",
    "                r.append(0)\r\n",
    "\r\n",
    "        ap = round(average_precision(r), 5)\r\n",
    "        print(f\"query: {query_original}\".ljust(100, \" \"), f\"Average Precision {ap}\")\r\n",
    "        rs.append(r)\r\n",
    "\r\n",
    "    map = mean_average_precision(rs)\r\n",
    "    #print(\"Mean Average Precision on validation query: \", map)\r\n",
    "    return map"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit"
  },
  "interpreter": {
   "hash": "2647ea34e536f865ab67ff9ddee7fd78773d956cec0cab53c79b32cd10da5d83"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}